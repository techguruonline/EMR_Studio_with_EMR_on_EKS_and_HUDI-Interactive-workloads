{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c4b406-aa65-4e7c-bb44-10bbaaa166f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69903430-bcbc-450c-a934-4c32e72175ca",
   "metadata": {},
   "source": [
    "Hudi is a rich platform to build streaming data lakes with incremental data pipelines on a self-managing database layer, while being optimized for lake engines and regular batch processing.<br>\n",
    "With ever growing data variety and volume, Data lakes became popular as a centralized repository that allows you to store all structured and unstructured data at any scale. Data lakes suffices the storage requirements and provides analytical capabilities, but doesn’t address the “transactional” requirements like the DML operations, ACID transactions (Atomicity, Consistency, Isolation, Durability) with concurrent reads and writes uses cases. With consumer privacy laws like GDPR, CCPA bring in more requirements that challenge the traditional designs of the data lake. Additionally, with businesses evolving there have been greater demands expected out of datalakes like the ability to apply Change Data Capture (CDC) at low latnecies, ability to rollback, travel back in time for point in time queries.<br>\n",
    "\n",
    "__Apache Hudi__ is a lakehouse, data platform technology that provides an incremental processing framework to power business critical data pipelines combining the benefits of stream and batch processing at low letancy and high performance. Hudi provides felxibility to choose queries engines like Spark, Presto, Hive, Trino, Amazon Athena and build pipelines using Spark, Flink, Hive.<br>\n",
    "\n",
    "Apache Hudi table format offers similar capabilities and functionalities that a traditional RDBMS provides but in a fully open table format so multiple engines like Spark, Trino, Presto etc can operate on the same dataset. It provides powerful features such as<br>\n",
    "\n",
    "- DML operarations such as Upsert, Deletes\n",
    "- Powerful feature to travel back in time to query point in time data to access sequential audit log of actions performed on the table\n",
    "- Transactions, Rollbacks, Concurrency Control, read and write from multiple applications concurrently\n",
    "- Automatic file sizing, data clustering, compactions, cleaning for efficient storage management of data and metadata\n",
    "- Streaming ingestion, Built-in CDC sources & tools, ability to apply change data capture incrementally.\n",
    "\n",
    "For more information, refer to [Apache Hudi Documentation](https://hudi.apache.org/docs/overview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e57c00a-2f44-47ff-9eb8-37ff7e92491b",
   "metadata": {},
   "source": [
    "## Pre-Requisite:\n",
    "For executing the code in this notebook you will need the below:<br>\n",
    "\n",
    "- A AWS account <br>\n",
    "\n",
    "__Below services should be created and configured__<br>\n",
    "\n",
    "- EMR Studio\n",
    "- EMR Studio Workspace\n",
    "- EMR on EKS Virtual Cluster\n",
    "- EKS Cluster (EC2 based)\n",
    "- Managed Endpoint (Hudi options configured with in the managed endpoint)\n",
    "- IAM Policy\n",
    "- Application Load Balancer\n",
    "- VPC and Subnet\n",
    "\n",
    "__<font color=red>_Important step for Iceberg to work with EMR Studio (Jupyter Notebook - JUPYTER_ENTERPRISE_GATEWAY) on EMR-on-EKS cluster_</font>__\n",
    "\n",
    "As Jupyter notebook (attached to EMR-on-EKS) does not support cell magic, we do not have an option to configure Spark specific parameters including Hudi Jars. For this reason we need to configure it as part of the Managed endpoint, below is the config. These are defaults, you can change the values according to your needs for e.g: increase executor.memory from 2G to 4G or 8G whatever you like.\n",
    "\n",
    "\n",
    "    \"configurationOverrides\": {\n",
    "        \"applicationConfiguration\": [\n",
    "            {\n",
    "                \"classification\": \"spark-defaults\",\n",
    "                \"properties\": {\n",
    "                    \"spark.executor.memory\": \"2G\",\n",
    "                    \"spark.driver.memory\": \"2G\",\n",
    "                    \"spark.sql.hive.convertMetastoreParquet\": \"false\",\n",
    "                    \"spark.kubernetes.executor.request.cores\": \"1.5\",\n",
    "                    \"spark.driver.cores\": \"1\",\n",
    "                    \"spark.sql.catalogImplementation\": \"hive\",\n",
    "                    \"spark.executor.cores\": \"1\",\n",
    "                    \"spark.dynamicAllocation.maxExecutors\": \"20\",\n",
    "                    \"spark.dynamicAllocation.shuffleTracking.enabled\": \"true\",\n",
    "                    \"spark.dynamicAllocation.shuffleTracking.timeout\": \"300s\",\n",
    "                    \"spark.kubernetes.driver.request.cores\": \"0.5\",\n",
    "                    \"spark.kubernetes.allocation.batch.size\": \"2\",\n",
    "                    \"spark.hadoop.hive.metastore.client.factory.class\": \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\",\n",
    "                    \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "                    \"spark.dynamicAllocation.minExecutors\": \"0\",\n",
    "                    \"spark.dynamicAllocation.enabled\": \"true\",\n",
    "                    \"spark.dynamicAllocation.executorAllocationRatio\": \"1\",\n",
    "                    \"spark.jars\": \"local:///usr/lib/hudi/hudi-spark-bundle.jar,local:///usr/lib/spark/external/lib/spark-avro.jar\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "\n",
    "### Current Setup used for this notebook\n",
    "- EMR version: __emr-6.8.0-latest__\n",
    "- EKS version: __1.21__\n",
    "- Instance Type for EKS cluster: __m5.xlarge__\n",
    "- No of Instances: __3__\n",
    "- Hudi Version: __0.11.1-amzn-0__\n",
    "- Spark Version: __3.3.0__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd4b7053-cd37-479f-887f-0278141efac9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:06:55.504693Z",
     "iopub.status.busy": "2022-11-28T15:06:55.504278Z",
     "iopub.status.idle": "2022-11-28T15:06:55.508904Z",
     "shell.execute_reply": "2022-11-28T15:06:55.508203Z",
     "shell.execute_reply.started": "2022-11-28T15:06:55.504666Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import few libraries \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e21e5e-9d29-4aa2-889b-811738cbfc95",
   "metadata": {},
   "source": [
    "## Create Dataframe and define Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64be65f-8260-47bd-81d2-fd1c26e9b9ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:06:58.054897Z",
     "iopub.status.busy": "2022-11-28T15:06:58.054402Z",
     "iopub.status.idle": "2022-11-28T15:06:58.413416Z",
     "shell.execute_reply": "2022-11-28T15:06:58.412399Z",
     "shell.execute_reply.started": "2022-11-28T15:06:58.054853Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "InputData = [\n",
    "    (1,'Prasad Nadig', 25, 'NJ','2022-01-01', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (2,'Ethereum', 80, 'NY', '2022-01-02', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (3,'Cosmos', 25, 'PA', '2022-01-03', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (4,'Solana', 55, 'MD', '2022-01-04', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (5,'Cardano', 15, 'TX', '2022-01-05', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (6,'Link', 45, 'NJ', '2022-01-06', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\"))\n",
    "]\n",
    "\n",
    "#Define schema for the source data\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cust_id\",IntegerType(),True), \\\n",
    "    StructField(\"cust_name\",StringType(),True), \\\n",
    "    StructField(\"cust_age\",IntegerType(),True), \\\n",
    "    StructField(\"cust_loc\",StringType(),True), \\\n",
    "    StructField(\"create_date\", StringType(), True), \\\n",
    "    StructField(\"last_updated_time\", TimestampType(), True)\n",
    "  ])\n",
    "\n",
    "#Create dataframe from the input data and the corresponding schema\n",
    "inputDF = spark.createDataFrame(data=InputData,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b7cb5f-b075-4353-bdaa-392cf0e49d25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:07:01.823271Z",
     "iopub.status.busy": "2022-11-28T15:07:01.822668Z",
     "iopub.status.idle": "2022-11-28T15:07:03.094465Z",
     "shell.execute_reply": "2022-11-28T15:07:03.093697Z",
     "shell.execute_reply.started": "2022-11-28T15:07:01.823224Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      25|      NJ| 2022-01-01|2022-11-28 15:06:58|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-11-28 15:06:58|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-11-28 15:06:58|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-11-28 15:06:58|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-11-28 15:06:58|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-11-28 15:06:58|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check data \n",
    "inputDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3a5f6-fd03-4072-a8d5-11c57fe2285e",
   "metadata": {},
   "source": [
    "## Define HUDI options, write data to S3 as HUDI dataset - INSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88bcc300-1521-45b6-b37e-d1864f18e0db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T05:57:11.341078Z",
     "iopub.status.busy": "2022-11-29T05:57:11.340628Z",
     "iopub.status.idle": "2022-11-29T05:57:11.345871Z",
     "shell.execute_reply": "2022-11-29T05:57:11.345074Z",
     "shell.execute_reply.started": "2022-11-29T05:57:11.341047Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Hudi options that we will pass below while writing the data to S3 as HUDI dataset\n",
    "# Hudi provides many other options that you can define as per your use case, refer to __[HUDI doc](https://hudi.apache.org/docs/configurations/)__ for more details\n",
    "# Ensure partitionpath.field and partition_fields have the same value to be in sync\n",
    "\n",
    "hudiOptions = {\n",
    "'hoodie.table.name': 'customer',\n",
    "'hoodie.datasource.write.recordkey.field': 'cust_id',\n",
    "'hoodie.datasource.write.partitionpath.field': 'create_date',\n",
    "'hoodie.datasource.write.precombine.field': 'last_updated_time',\n",
    "'hoodie.datasource.hive_sync.enable': 'true',\n",
    "'hoodie.datasource.hive_sync.use_jdbc': 'false',\n",
    "'hoodie.datasource.hive_sync.mode':'hms',\n",
    "'hoodie.datasource.hive_sync.table': 'customer',\n",
    "'hoodie.datasource.hive_sync.partition_fields': 'create_date',\n",
    "'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47488b86-fbe5-4649-8913-a8f687e85367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:01:37.912567Z",
     "iopub.status.busy": "2022-11-29T06:01:37.912136Z",
     "iopub.status.idle": "2022-11-29T06:02:06.359940Z",
     "shell.execute_reply": "2022-11-29T06:02:06.359064Z",
     "shell.execute_reply.started": "2022-11-29T06:01:37.912537Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write dataframe that we created above to S3 as HUDI dataset. \n",
    "# As this is our first write, we will use write.operation as 'Insert' and .mode as 'overwrite'\n",
    "\n",
    "inputDF.write \\\n",
    ".format('org.apache.hudi') \\\n",
    ".option('hoodie.datasource.write.operation', 'insert') \\\n",
    ".options(**hudiOptions) \\\n",
    ".mode('overwrite') \\\n",
    ".save('s3://emr-studio-emr-on-eks/hudi-tables/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298aa66f-c46a-4220-b318-379e54245bc4",
   "metadata": {},
   "source": [
    "## Read data from HUDI Dataset we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f99bdff3-e1e0-4c22-a533-94e6f99cb5bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:13:34.268769Z",
     "iopub.status.busy": "2022-11-29T06:13:34.268331Z",
     "iopub.status.idle": "2022-11-29T06:13:48.490073Z",
     "shell.execute_reply": "2022-11-29T06:13:48.489279Z",
     "shell.execute_reply.started": "2022-11-29T06:13:34.268739Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      25|      NJ| 2022-01-01|2022-11-29 05:35:55|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-11-29 05:35:55|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-11-29 05:35:55|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-11-29 05:35:55|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-11-29 05:35:55|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-11-29 05:35:55|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By default HUDI performs snapshot queries. \n",
    "snapshotQueryDF = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*')\n",
    "    \n",
    "snapshotQueryDF.select(\"cust_id\", \"cust_name\", \"cust_age\", \"cust_loc\", \"create_date\", \"last_updated_time\").orderBy(\"cust_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438da65-cb0f-4e02-a09d-aa96313261a1",
   "metadata": {},
   "source": [
    "# DML Operations\n",
    "\n",
    "## UPSERT\n",
    "###  - HUDI write operation provides 3 options Upsert/Insert and Bulk Insert, we did Insert in the previous steps, now lets try the upsert operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "743fc35e-6a4b-49b4-9fd8-ba39bd3684de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:16:37.876238Z",
     "iopub.status.busy": "2022-11-29T06:16:37.875808Z",
     "iopub.status.idle": "2022-11-29T06:16:37.948755Z",
     "shell.execute_reply": "2022-11-29T06:16:37.947914Z",
     "shell.execute_reply.started": "2022-11-29T06:16:37.876208Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will update an existing record and insert a new record. Upsert operation in HUDI will find the record based on the RecordKey, if found it will update the value, if not found then will Insert the record.\n",
    "InputData = [\n",
    "    (1,'Prasad S Nadig', 30, 'NJ','2022-01-01', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")), #Update\n",
    "    (7,'Compound', 20, 'NJ', '2022-01-07', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")) #Insert\n",
    "]\n",
    "\n",
    "# Define schema for the source data\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cust_id\",IntegerType(),True), \\\n",
    "    StructField(\"cust_name\",StringType(),True), \\\n",
    "    StructField(\"cust_age\",IntegerType(),True), \\\n",
    "    StructField(\"cust_loc\",StringType(),True), \\\n",
    "    StructField(\"create_date\", StringType(), True), \\\n",
    "    StructField(\"last_updated_time\", TimestampType(), True)\n",
    "  ])\n",
    "\n",
    "# Create dataframe from the input data and the corresponding schema\n",
    "updateDF = spark.createDataFrame(data=InputData,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0109bc6d-deec-4dc1-bbaa-13c2f852e6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:16:53.792605Z",
     "iopub.status.busy": "2022-11-29T06:16:53.792152Z",
     "iopub.status.idle": "2022-11-29T06:17:35.498089Z",
     "shell.execute_reply": "2022-11-29T06:17:35.497116Z",
     "shell.execute_reply.started": "2022-11-29T06:16:53.792575Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we will Update/Insert the data to HUDI dataset on S3, instead of insert, we will use \"upsert\" for write.operation and instead of overwrite, we will use \"append\" for .mode\n",
    "\n",
    "updateDF.write \\\n",
    ".format('org.apache.hudi') \\\n",
    ".option('hoodie.datasource.write.operation', 'upsert') \\\n",
    ".options(**hudiOptions) \\\n",
    ".mode('append') \\\n",
    ".save('s3://emr-studio-emr-on-eks/hudi-tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7dbd3de-6dd4-4157-8f6c-c846a2e1822b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:23:07.148397Z",
     "iopub.status.busy": "2022-11-29T06:23:07.147957Z",
     "iopub.status.idle": "2022-11-29T06:23:21.357629Z",
     "shell.execute_reply": "2022-11-29T06:23:21.356720Z",
     "shell.execute_reply.started": "2022-11-29T06:23:07.148366Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|     cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad S Nadig|      30|      NJ| 2022-01-01|2022-11-29 06:16:37|\n",
      "|      2|      Ethereum|      80|      NY| 2022-01-02|2022-11-29 05:35:55|\n",
      "|      3|        Cosmos|      25|      PA| 2022-01-03|2022-11-29 05:35:55|\n",
      "|      4|        Solana|      55|      MD| 2022-01-04|2022-11-29 05:35:55|\n",
      "|      5|       Carnado|      15|      TX| 2022-01-05|2022-11-29 05:35:55|\n",
      "|      6|          Link|      45|      NJ| 2022-01-06|2022-11-29 05:35:55|\n",
      "|      7|      Compound|      20|      NJ| 2022-01-07|2022-11-29 06:16:37|\n",
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exisitng record should now be updated and the new record should be inserted in the HUDI dataset\n",
    "# You should see cust_name and cust_age for cust_id=1 is updated and a new record cust_id=7 is inserted.\n",
    "# also notice that the last_updated_time is also updated for cust_id=1\n",
    "\n",
    "snapshotQueryDF = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*')\n",
    "    \n",
    "snapshotQueryDF.select(\"cust_id\", \"cust_name\", \"cust_age\", \"cust_loc\", \"create_date\", \"last_updated_time\").orderBy(\"cust_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468d07e-3122-4006-a104-985961b0b022",
   "metadata": {},
   "source": [
    "## DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9446a03d-938e-46f4-9cde-3600873af12a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:26:16.347874Z",
     "iopub.status.busy": "2022-11-29T06:26:16.347170Z",
     "iopub.status.idle": "2022-11-29T06:26:16.357691Z",
     "shell.execute_reply": "2022-11-29T06:26:16.356982Z",
     "shell.execute_reply.started": "2022-11-29T06:26:16.347840Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HUDI allows you to delete records just like traditional RDBMS, so let's delete a record\n",
    "deleteDF = snapshotQueryDF.where(\"cust_id==6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3948beab-bc8c-43b5-99f8-d57db1f45218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:26:21.148615Z",
     "iopub.status.busy": "2022-11-29T06:26:21.148175Z",
     "iopub.status.idle": "2022-11-29T06:27:03.943720Z",
     "shell.execute_reply": "2022-11-29T06:27:03.942739Z",
     "shell.execute_reply.started": "2022-11-29T06:26:21.148586Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write to HUDI dataset to apply the deletes\n",
    "# We will pass empty payload to permanently delete the record from the HUDI dataset\n",
    "\n",
    "deleteDF.write \\\n",
    ".format('org.apache.hudi') \\\n",
    ".option(\"hoodie.datasource.write.payload.class\", \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\") \\\n",
    ".options(**hudiOptions) \\\n",
    ".mode('append') \\\n",
    ".save('s3://emr-studio-emr-on-eks/hudi-tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a99bd9dd-cc11-4166-9ed4-cfb7f74ecb9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:29:56.969440Z",
     "iopub.status.busy": "2022-11-29T06:29:56.969013Z",
     "iopub.status.idle": "2022-11-29T06:30:25.923192Z",
     "shell.execute_reply": "2022-11-29T06:30:25.922277Z",
     "shell.execute_reply.started": "2022-11-29T06:29:56.969411Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|     cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad S Nadig|      30|      NJ| 2022-01-01|2022-11-29 06:16:37|\n",
      "|      2|      Ethereum|      80|      NY| 2022-01-02|2022-11-29 05:35:55|\n",
      "|      3|        Cosmos|      25|      PA| 2022-01-03|2022-11-29 05:35:55|\n",
      "|      4|        Solana|      55|      MD| 2022-01-04|2022-11-29 05:35:55|\n",
      "|      5|       Carnado|      15|      TX| 2022-01-05|2022-11-29 05:35:55|\n",
      "|      7|      Compound|      20|      NJ| 2022-01-07|2022-11-29 06:16:37|\n",
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notice that cust_id=6 has been permanently deleted from the dataset\n",
    "deleteReadDF = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*')\n",
    "    \n",
    "deleteReadDF.select(\"cust_id\", \"cust_name\", \"cust_age\", \"cust_loc\", \"create_date\", \"last_updated_time\").orderBy(\"cust_id\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ba928-156f-4086-b2f6-e85262101ea0",
   "metadata": {},
   "source": [
    "## Time Travel - Point in time Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f159734c-4536-47c6-985a-7a9e4f49a044",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:30:38.539040Z",
     "iopub.status.busy": "2022-11-29T06:30:38.538608Z",
     "iopub.status.idle": "2022-11-29T06:30:40.472587Z",
     "shell.execute_reply": "2022-11-29T06:30:40.471333Z",
     "shell.execute_reply.started": "2022-11-29T06:30:38.539009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read  \\\n",
    "    .format(\"hudi\") \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*') \\\n",
    "    .createOrReplaceTempView(\"hudi_snapshot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008eabe-4f90-412e-813e-abc8c2781929",
   "metadata": {},
   "source": [
    "### Commit time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80a8bc50-9ba4-4284-a0ac-a1ba0b54885a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:30:50.883636Z",
     "iopub.status.busy": "2022-11-29T06:30:50.883140Z",
     "iopub.status.idle": "2022-11-29T06:30:56.531020Z",
     "shell.execute_reply": "2022-11-29T06:30:56.529978Z",
     "shell.execute_reply.started": "2022-11-29T06:30:50.883605Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+----------+\n",
      "|       commitTime|primaryKey| partition|\n",
      "+-----------------+----------+----------+\n",
      "|20221129060138055|         2|2022-01-02|\n",
      "|20221129060138055|         5|2022-01-05|\n",
      "|20221129060138055|         3|2022-01-03|\n",
      "|20221129060138055|         4|2022-01-04|\n",
      "|20221129061653922|         1|2022-01-01|\n",
      "|20221129061653922|         7|2022-01-07|\n",
      "+-----------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hudi stores commit time for each DML operation performed on the \"recordKey\"in its metadata files. \n",
    "# Below query will fetch commit time for INSERT and for UPSERT by recordKey\n",
    "spark.sql(\"select _hoodie_commit_time as commitTime, _hoodie_record_key as primaryKey, _hoodie_partition_path as partition from  hudi_snapshot order by commitTime\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2549f9a6-0911-45aa-86ac-585f54776ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:31:03.661871Z",
     "iopub.status.busy": "2022-11-29T06:31:03.661426Z",
     "iopub.status.idle": "2022-11-29T06:31:05.122911Z",
     "shell.execute_reply": "2022-11-29T06:31:05.122089Z",
     "shell.execute_reply.started": "2022-11-29T06:31:03.661841Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lets get distinct commit time from the Hudi dataset\n",
    "commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_snapshot order by commitTime\").limit(50).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2287b94-8113-4564-aa40-1803353f3c8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:31:08.111639Z",
     "iopub.status.busy": "2022-11-29T06:31:08.110971Z",
     "iopub.status.idle": "2022-11-29T06:31:08.117315Z",
     "shell.execute_reply": "2022-11-29T06:31:08.116495Z",
     "shell.execute_reply.started": "2022-11-29T06:31:08.111584Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20221129060138055', '20221129061653922']\n"
     ]
    }
   ],
   "source": [
    "#print to verify the distinct values for commits\n",
    "print(commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e77e0f96-3e5f-4603-bc5c-10d9faa661f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:31:13.553127Z",
     "iopub.status.busy": "2022-11-29T06:31:13.552709Z",
     "iopub.status.idle": "2022-11-29T06:31:20.030271Z",
     "shell.execute_reply": "2022-11-29T06:31:20.029076Z",
     "shell.execute_reply.started": "2022-11-29T06:31:13.553093Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+------------+--------+--------+-----------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+------------+--------+--------+-----------+-------------------+\n",
      "|  20221129060138055|20221129060138055...|                 1|            2022-01-01|42bbae9a-3144-455...|      1|Prasad Nadig|      25|      NJ| 2022-01-01|2022-11-29 05:35:55|\n",
      "|  20221129060138055|20221129060138055...|                 2|            2022-01-02|f0f97bb9-03aa-4b5...|      2|    Ethereum|      80|      NY| 2022-01-02|2022-11-29 05:35:55|\n",
      "|  20221129060138055|20221129060138055...|                 5|            2022-01-05|7afe7c55-79af-4b1...|      5|     Carnado|      15|      TX| 2022-01-05|2022-11-29 05:35:55|\n",
      "|  20221129060138055|20221129060138055...|                 3|            2022-01-03|e7546add-f487-438...|      3|      Cosmos|      25|      PA| 2022-01-03|2022-11-29 05:35:55|\n",
      "|  20221129060138055|20221129060138055...|                 4|            2022-01-04|a6778de2-bd34-45e...|      4|      Solana|      55|      MD| 2022-01-04|2022-11-29 05:35:55|\n",
      "|  20221129060138055|20221129060138055...|                 6|            2022-01-06|77e7d127-88db-432...|      6|        Link|      45|      NJ| 2022-01-06|2022-11-29 05:35:55|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set parameters for Hudi options\n",
    "startTime = \"000\" # Fetches all available commits since start.\n",
    "endTime = commits[len(commits) - 2] # Fetches the initial commit time\n",
    "\n",
    "# Define Hudi options to query point in time data with a start and end time\n",
    "time_travel_options = {\n",
    "    'hoodie.datasource.query.type': 'incremental',\n",
    "    'hoodie.datasource.read.end.instanttime': endTime,\n",
    "    'hoodie.datasource.read.begin.instanttime': startTime\n",
    "}\n",
    "\n",
    "# get the initial table before upsert and delete (Original Inserts)\n",
    "df_time_travel_read = spark.read.format(\"hudi\") \\\n",
    "    .options(**time_travel_options)  \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a4cb6-4c20-4ab5-b03d-131e762750eb",
   "metadata": {},
   "source": [
    "### Incremental Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2102de4d-4c5e-40ad-a33e-12d64fc7237e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T06:31:37.945248Z",
     "iopub.status.busy": "2022-11-29T06:31:37.944814Z",
     "iopub.status.idle": "2022-11-29T06:31:39.587978Z",
     "shell.execute_reply": "2022-11-29T06:31:39.586896Z",
     "shell.execute_reply.started": "2022-11-29T06:31:37.945219Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|cust_id|     cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|  20221129061653922|20221129061653922...|                 1|            2022-01-01|42bbae9a-3144-455...|      1|Prasad S Nadig|      30|      NJ| 2022-01-01|2022-11-29 06:16:37|\n",
      "|  20221129061653922|20221129061653922...|                 7|            2022-01-07|3477b517-c353-454...|      7|      Compound|      20|      NJ| 2022-01-07|2022-11-29 06:16:37|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startTime = commits[len(commits) - 2] # fetch commit time for incremental data (UPSERT)\n",
    "\n",
    "# fetch incremental data after initial insert, startTime represents the commit time for UPSERT\n",
    "incremental_read_options = {\n",
    "    'hoodie.datasource.query.type': 'incremental',\n",
    "    'hoodie.datasource.read.begin.instanttime': startTime\n",
    "}\n",
    "\n",
    "df_customer_incremental_read = spark.read.format(\"hudi\") \\\n",
    "    .options(**incremental_read_options)  \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49158294-7c73-45c9-a00a-1a2eb0617932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Kubernetes)",
   "language": "python",
   "name": "spark_python_kubernetes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
