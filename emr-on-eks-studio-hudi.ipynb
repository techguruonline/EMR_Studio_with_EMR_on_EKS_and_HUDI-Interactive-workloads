{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd4b7053-cd37-479f-887f-0278141efac9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:06:55.504693Z",
     "iopub.status.busy": "2022-11-28T15:06:55.504278Z",
     "iopub.status.idle": "2022-11-28T15:06:55.508904Z",
     "shell.execute_reply": "2022-11-28T15:06:55.508203Z",
     "shell.execute_reply.started": "2022-11-28T15:06:55.504666Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e21e5e-9d29-4aa2-889b-811738cbfc95",
   "metadata": {},
   "source": [
    "## Create Dataframe and define Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64be65f-8260-47bd-81d2-fd1c26e9b9ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:06:58.054897Z",
     "iopub.status.busy": "2022-11-28T15:06:58.054402Z",
     "iopub.status.idle": "2022-11-28T15:06:58.413416Z",
     "shell.execute_reply": "2022-11-28T15:06:58.412399Z",
     "shell.execute_reply.started": "2022-11-28T15:06:58.054853Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "InputData = [\n",
    "    (1,'Prasad Nadig', 25, 'NJ','2022-01-01', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (2,'Ethereum', 80, 'NY', '2022-01-02', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (3,'Cosmos', 25, 'PA', '2022-01-03', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (4,'Solana', 55, 'MD', '2022-01-04', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (5,'Carnado', 15, 'TX', '2022-01-05', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (6,'Link', 45, 'NJ', '2022-01-06', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\"))\n",
    "]\n",
    "\n",
    "#Define schema for the source data\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cust_id\",IntegerType(),True), \\\n",
    "    StructField(\"cust_name\",StringType(),True), \\\n",
    "    StructField(\"cust_age\",IntegerType(),True), \\\n",
    "    StructField(\"cust_loc\",StringType(),True), \\\n",
    "    StructField(\"create_date\", StringType(), True), \\\n",
    "    StructField(\"last_updated_time\", TimestampType(), True)\n",
    "  ])\n",
    "\n",
    "#Create dataframe from the input data and the corresponding schema\n",
    "inputDF = spark.createDataFrame(data=InputData,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08b7cb5f-b075-4353-bdaa-392cf0e49d25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:07:01.823271Z",
     "iopub.status.busy": "2022-11-28T15:07:01.822668Z",
     "iopub.status.idle": "2022-11-28T15:07:03.094465Z",
     "shell.execute_reply": "2022-11-28T15:07:03.093697Z",
     "shell.execute_reply.started": "2022-11-28T15:07:01.823224Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      25|      NJ| 2022-01-01|2022-11-28 15:06:58|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-11-28 15:06:58|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-11-28 15:06:58|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-11-28 15:06:58|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-11-28 15:06:58|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-11-28 15:06:58|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check data \n",
    "inputDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3a5f6-fd03-4072-a8d5-11c57fe2285e",
   "metadata": {},
   "source": [
    "## Define HUDI options, write data to S3 as HUDI dataset - INSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88bcc300-1521-45b6-b37e-d1864f18e0db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:07:05.460566Z",
     "iopub.status.busy": "2022-11-28T15:07:05.460127Z",
     "iopub.status.idle": "2022-11-28T15:07:05.465263Z",
     "shell.execute_reply": "2022-11-28T15:07:05.464501Z",
     "shell.execute_reply.started": "2022-11-28T15:07:05.460536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hudiOptions = {\n",
    "'hoodie.table.name': 'customer',\n",
    "'hoodie.datasource.write.recordkey.field': 'cust_id',\n",
    "'hoodie.datasource.write.partitionpath.field': 'create_date',\n",
    "'hoodie.datasource.write.precombine.field': 'last_updated_time',\n",
    "'hoodie.datasource.hive_sync.enable': 'true',\n",
    "'hoodie.datasource.hive_sync.use_jdbc': 'false',\n",
    "'hoodie.datasource.hive_sync.mode':'hms',\n",
    "'hoodie.datasource.hive_sync.table': 'customer',\n",
    "'hoodie.datasource.hive_sync.partition_fields': 'create_date',\n",
    "'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47488b86-fbe5-4649-8913-a8f687e85367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:07:07.852873Z",
     "iopub.status.busy": "2022-11-28T15:07:07.852443Z",
     "iopub.status.idle": "2022-11-28T15:07:45.787959Z",
     "shell.execute_reply": "2022-11-28T15:07:45.787020Z",
     "shell.execute_reply.started": "2022-11-28T15:07:07.852845Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputDF.write \\\n",
    ".format('org.apache.hudi') \\\n",
    ".option('hoodie.datasource.write.operation', 'insert') \\\n",
    ".options(**hudiOptions) \\\n",
    ".mode('overwrite') \\\n",
    ".save('s3://emr-studio-emr-on-eks/hudi-tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a6548-f910-443d-b631-9361aa100cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298aa66f-c46a-4220-b318-379e54245bc4",
   "metadata": {},
   "source": [
    "## Read data from HUDI Dataset we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f99bdff3-e1e0-4c22-a533-94e6f99cb5bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:07:49.661668Z",
     "iopub.status.busy": "2022-11-28T15:07:49.661179Z",
     "iopub.status.idle": "2022-11-28T15:07:54.634007Z",
     "shell.execute_reply": "2022-11-28T15:07:54.632957Z",
     "shell.execute_reply.started": "2022-11-28T15:07:49.661637Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      25|      NJ| 2022-01-01|2022-11-28 15:06:58|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-11-28 15:06:58|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-11-28 15:06:58|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-11-28 15:06:58|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-11-28 15:06:58|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-11-28 15:06:58|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By default HUDI performs snapshot queries. \n",
    "snapshotQueryDF = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*')\n",
    "    \n",
    "snapshotQueryDF.select(\"cust_id\", \"cust_name\", \"cust_age\", \"cust_loc\", \"create_date\", \"last_updated_time\").orderBy(\"cust_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438da65-cb0f-4e02-a09d-aa96313261a1",
   "metadata": {},
   "source": [
    "# DML Operations\n",
    "\n",
    "## UPSERT\n",
    "###  - HUDI write opearation provides 3 options Upsert/Insert and Bulk Insert, we did Insert in the previous steps, now lets try the upsert operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "743fc35e-6a4b-49b4-9fd8-ba39bd3684de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:07:58.074137Z",
     "iopub.status.busy": "2022-11-28T15:07:58.073715Z",
     "iopub.status.idle": "2022-11-28T15:07:58.104952Z",
     "shell.execute_reply": "2022-11-28T15:07:58.103876Z",
     "shell.execute_reply.started": "2022-11-28T15:07:58.074107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will update an existing record and insert a new record. Upsert operation in HUDI will find the record based on the RecordKey, if found it will update the value, if not found then will Insert the record.\n",
    "InputData = [\n",
    "    (1,'Prasad S Nadig', 30, 'NJ','2022-01-01', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")), #Update\n",
    "    (7,'Compound', 20, 'NJ', '2022-01-07', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")) #Insert\n",
    "]\n",
    "\n",
    "#Define schema for the source data\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cust_id\",IntegerType(),True), \\\n",
    "    StructField(\"cust_name\",StringType(),True), \\\n",
    "    StructField(\"cust_age\",IntegerType(),True), \\\n",
    "    StructField(\"cust_loc\",StringType(),True), \\\n",
    "    StructField(\"create_date\", StringType(), True), \\\n",
    "    StructField(\"last_updated_time\", TimestampType(), True)\n",
    "  ])\n",
    "\n",
    "#Create dataframe from the input data and the corresponding schema\n",
    "updateDF = spark.createDataFrame(data=InputData,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0109bc6d-deec-4dc1-bbaa-13c2f852e6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:08:01.448514Z",
     "iopub.status.busy": "2022-11-28T15:08:01.448070Z",
     "iopub.status.idle": "2022-11-28T15:08:28.932645Z",
     "shell.execute_reply": "2022-11-28T15:08:28.931731Z",
     "shell.execute_reply.started": "2022-11-28T15:08:01.448484Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now we will Update/Insert the data to HUDI dataset on S3, instead of insert, we will use \"upsert\" and instead of overwrite for mode, we will use \"append\"\n",
    "\n",
    "updateDF.write \\\n",
    ".format('org.apache.hudi') \\\n",
    ".option('hoodie.datasource.write.operation', 'upsert') \\\n",
    ".options(**hudiOptions) \\\n",
    ".mode('append') \\\n",
    ".save('s3://emr-studio-emr-on-eks/hudi-tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7dbd3de-6dd4-4157-8f6c-c846a2e1822b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:08:32.966429Z",
     "iopub.status.busy": "2022-11-28T15:08:32.965971Z",
     "iopub.status.idle": "2022-11-28T15:08:35.917571Z",
     "shell.execute_reply": "2022-11-28T15:08:35.916682Z",
     "shell.execute_reply.started": "2022-11-28T15:08:32.966401Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|     cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad S Nadig|      30|      NJ| 2022-01-01|2022-11-28 15:07:58|\n",
      "|      2|      Ethereum|      80|      NY| 2022-01-02|2022-11-28 15:06:58|\n",
      "|      3|        Cosmos|      25|      PA| 2022-01-03|2022-11-28 15:06:58|\n",
      "|      4|        Solana|      55|      MD| 2022-01-04|2022-11-28 15:06:58|\n",
      "|      5|       Carnado|      15|      TX| 2022-01-05|2022-11-28 15:06:58|\n",
      "|      6|          Link|      45|      NJ| 2022-01-06|2022-11-28 15:06:58|\n",
      "|      7|      Compound|      20|      NJ| 2022-01-07|2022-11-28 15:07:58|\n",
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets check the HUDI dataset if the record was updated and the new record was inserted or not\n",
    "# You should see cust_name and cust_age for cust_i=1 is updated and a new record cust_id=7 is inserted.\n",
    "#also notice that the last_updated_time is also updated for cust_id=1\n",
    "\n",
    "snapshotQueryDF = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*')\n",
    "    \n",
    "snapshotQueryDF.select(\"cust_id\", \"cust_name\", \"cust_age\", \"cust_loc\", \"create_date\", \"last_updated_time\").orderBy(\"cust_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468d07e-3122-4006-a104-985961b0b022",
   "metadata": {},
   "source": [
    "## DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9446a03d-938e-46f4-9cde-3600873af12a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:08:40.377407Z",
     "iopub.status.busy": "2022-11-28T15:08:40.376960Z",
     "iopub.status.idle": "2022-11-28T15:08:40.389558Z",
     "shell.execute_reply": "2022-11-28T15:08:40.388312Z",
     "shell.execute_reply.started": "2022-11-28T15:08:40.377381Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#HUDI alllows you to delete records just like traditional RDBMS, so let's delete a record\n",
    "deleteDF = snapshotQueryDF.where(\"cust_id==6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3948beab-bc8c-43b5-99f8-d57db1f45218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:08:42.897742Z",
     "iopub.status.busy": "2022-11-28T15:08:42.897333Z",
     "iopub.status.idle": "2022-11-28T15:09:05.647362Z",
     "shell.execute_reply": "2022-11-28T15:09:05.646487Z",
     "shell.execute_reply.started": "2022-11-28T15:08:42.897715Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Write to HUDI dataset to apply the deletes\n",
    "deleteDF.write \\\n",
    ".format('org.apache.hudi') \\\n",
    ".option(\"hoodie.datasource.write.payload.class\", \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\") \\\n",
    ".options(**hudiOptions) \\\n",
    ".mode('append') \\\n",
    ".save('s3://emr-studio-emr-on-eks/hudi-tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a99bd9dd-cc11-4166-9ed4-cfb7f74ecb9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T15:09:07.949438Z",
     "iopub.status.busy": "2022-11-28T15:09:07.948982Z",
     "iopub.status.idle": "2022-11-28T15:09:11.130951Z",
     "shell.execute_reply": "2022-11-28T15:09:11.130084Z",
     "shell.execute_reply.started": "2022-11-28T15:09:07.949410Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|     cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad S Nadig|      30|      NJ| 2022-01-01|2022-11-28 15:07:58|\n",
      "|      2|      Ethereum|      80|      NY| 2022-01-02|2022-11-28 15:06:58|\n",
      "|      3|        Cosmos|      25|      PA| 2022-01-03|2022-11-28 15:06:58|\n",
      "|      4|        Solana|      55|      MD| 2022-01-04|2022-11-28 15:06:58|\n",
      "|      5|       Carnado|      15|      TX| 2022-01-05|2022-11-28 15:06:58|\n",
      "|      7|      Compound|      20|      NJ| 2022-01-07|2022-11-28 15:07:58|\n",
      "+-------+--------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Notice that cust_id=6 has been permanently deleted from the dataset\n",
    "deleteReadDF = spark.read \\\n",
    "    .format('org.apache.hudi') \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*')\n",
    "    \n",
    "deleteReadDF.select(\"cust_id\", \"cust_name\", \"cust_age\", \"cust_loc\", \"create_date\", \"last_updated_time\").orderBy(\"cust_id\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ba928-156f-4086-b2f6-e85262101ea0",
   "metadata": {},
   "source": [
    "## Time Travel - Point in time Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f159734c-4536-47c6-985a-7a9e4f49a044",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T05:11:01.387155Z",
     "iopub.status.busy": "2022-11-29T05:11:01.386727Z",
     "iopub.status.idle": "2022-11-29T05:11:21.574365Z",
     "shell.execute_reply": "2022-11-29T05:11:21.573259Z",
     "shell.execute_reply.started": "2022-11-29T05:11:01.387126Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read  \\\n",
    "    .format(\"hudi\") \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*') \\\n",
    "    .createOrReplaceTempView(\"hudi_snapshot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008eabe-4f90-412e-813e-abc8c2781929",
   "metadata": {},
   "source": [
    "### Commit time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80a8bc50-9ba4-4284-a0ac-a1ba0b54885a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T05:13:14.411246Z",
     "iopub.status.busy": "2022-11-29T05:13:14.410817Z",
     "iopub.status.idle": "2022-11-29T05:13:28.184538Z",
     "shell.execute_reply": "2022-11-29T05:13:28.183773Z",
     "shell.execute_reply.started": "2022-11-29T05:13:14.411214Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+----------+\n",
      "|       commitTime|primaryKey| partition|\n",
      "+-----------------+----------+----------+\n",
      "|20221128150707999|         2|2022-01-02|\n",
      "|20221128150707999|         3|2022-01-03|\n",
      "|20221128150707999|         4|2022-01-04|\n",
      "|20221128150707999|         5|2022-01-05|\n",
      "|20221128150801565|         1|2022-01-01|\n",
      "|20221128150801565|         7|2022-01-07|\n",
      "|20221128175805365|         8|2022-01-01|\n",
      "+-----------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hudi stores commit time for each DML operation performed on the \"recordKey\"in its metadata files. \n",
    "# Below query will fetch commit time for INSERT and for UPSERT by recordKey\n",
    "spark.sql(\"select _hoodie_commit_time as commitTime, _hoodie_record_key as primaryKey, _hoodie_partition_path as partition from  hudi_snapshot order by commitTime\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2549f9a6-0911-45aa-86ac-585f54776ec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T05:18:11.632343Z",
     "iopub.status.busy": "2022-11-29T05:18:11.631903Z",
     "iopub.status.idle": "2022-11-29T05:18:25.794917Z",
     "shell.execute_reply": "2022-11-29T05:18:25.793945Z",
     "shell.execute_reply.started": "2022-11-29T05:18:11.632313Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Lets get distinct commit time from the Hudi dataset\n",
    "commits = list(map(lambda row: row[0], spark.sql(\"select distinct(_hoodie_commit_time) as commitTime from  hudi_snapshot order by commitTime\").limit(50).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2287b94-8113-4564-aa40-1803353f3c8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T05:19:20.806207Z",
     "iopub.status.busy": "2022-11-29T05:19:20.805700Z",
     "iopub.status.idle": "2022-11-29T05:19:20.810690Z",
     "shell.execute_reply": "2022-11-29T05:19:20.809980Z",
     "shell.execute_reply.started": "2022-11-29T05:19:20.806175Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20221128150707999', '20221128150801565', '20221128175805365']\n"
     ]
    }
   ],
   "source": [
    "#print to verify the distinct values for commits\n",
    "print(commits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e77e0f96-3e5f-4603-bc5c-10d9faa661f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-29T05:18:28.308551Z",
     "iopub.status.busy": "2022-11-29T05:18:28.308107Z",
     "iopub.status.idle": "2022-11-29T05:18:33.734935Z",
     "shell.execute_reply": "2022-11-29T05:18:33.734157Z",
     "shell.execute_reply.started": "2022-11-29T05:18:28.308520Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+--------+--------+-----------+-------------------+--------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|cust_id|     cust_name|cust_age|cust_loc|create_date|  last_updated_time|phone_no|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+--------+--------+-----------+-------------------+--------+\n",
      "|  20221128150801565|20221128150801565...|                 1|            2022-01-01|a99af538-8324-4c1...|      1|Prasad S Nadig|      30|      NJ| 2022-01-01|2022-11-28 15:07:58|    null|\n",
      "|  20221128150801565|20221128150801565...|                 7|            2022-01-07|65af084c-7eff-40e...|      7|      Compound|      20|      NJ| 2022-01-07|2022-11-28 15:07:58|    null|\n",
      "|  20221128150707999|20221128150707999...|                 2|            2022-01-02|8d937f49-499a-452...|      2|      Ethereum|      80|      NY| 2022-01-02|2022-11-28 15:06:58|    null|\n",
      "|  20221128150707999|20221128150707999...|                 5|            2022-01-05|9b874bee-6fbb-44e...|      5|       Carnado|      15|      TX| 2022-01-05|2022-11-28 15:06:58|    null|\n",
      "|  20221128150707999|20221128150707999...|                 3|            2022-01-03|028ece2b-fa13-42e...|      3|        Cosmos|      25|      PA| 2022-01-03|2022-11-28 15:06:58|    null|\n",
      "|  20221128150707999|20221128150707999...|                 4|            2022-01-04|6cd03a1a-8981-4f2...|      4|        Solana|      55|      MD| 2022-01-04|2022-11-28 15:06:58|    null|\n",
      "|  20221128150707999|20221128150707999...|                 6|            2022-01-06|67839258-c40d-460...|      6|          Link|      45|      NJ| 2022-01-06|2022-11-28 15:06:58|    null|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+--------+--------+-----------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set parameters for Hudi options\n",
    "startTime = \"000\" # Fetches all available commits since start.\n",
    "endTime = commits[len(commits) - 2] # Fetches the initial commit time\n",
    "\n",
    "# Define Hudi options to query point in time data with a start and end time\n",
    "time_travel_options = {\n",
    "    'hoodie.datasource.query.type': 'incremental',\n",
    "    'hoodie.datasource.read.end.instanttime': endTime,\n",
    "    'hoodie.datasource.read.begin.instanttime': startTime\n",
    "}\n",
    "\n",
    "# get the initial table before upsert and delete (Original Inserts)\n",
    "df_time_travel_read = spark.read.format(\"hudi\") \\\n",
    "    .options(**time_travel_options)  \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294a4cb6-4c20-4ab5-b03d-131e762750eb",
   "metadata": {},
   "source": [
    "### Incremental Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2102de4d-4c5e-40ad-a33e-12d64fc7237e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T16:37:43.183320Z",
     "iopub.status.busy": "2022-11-28T16:37:43.182865Z",
     "iopub.status.idle": "2022-11-28T16:37:45.813117Z",
     "shell.execute_reply": "2022-11-28T16:37:45.812065Z",
     "shell.execute_reply.started": "2022-11-28T16:37:43.183291Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|cust_id|     cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+--------+--------+-----------+-------------------+\n",
      "|  20221128150801565|20221128150801565...|                 1|            2022-01-01|a99af538-8324-4c1...|      1|Prasad S Nadig|      30|      NJ| 2022-01-01|2022-11-28 15:07:58|\n",
      "|  20221128150801565|20221128150801565...|                 7|            2022-01-07|65af084c-7eff-40e...|      7|      Compound|      20|      NJ| 2022-01-07|2022-11-28 15:07:58|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------+--------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "startTime = commits[len(commits) - 2] # fetch commit time for incremental data (UPSERT)\n",
    "\n",
    "# fetch incremental data after initial insert, startTime represents the commit time for UPSERT\n",
    "incremental_read_options = {\n",
    "    'hoodie.datasource.query.type': 'incremental',\n",
    "    'hoodie.datasource.read.begin.instanttime': startTime\n",
    "}\n",
    "\n",
    "df_customer_incremental_read = spark.read.format(\"hudi\") \\\n",
    "    .options(**incremental_read_options)  \\\n",
    "    .load('s3://emr-studio-emr-on-eks/hudi-tables' + '/*/*') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e5842-0671-44b5-863d-e1837d32e196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Kubernetes)",
   "language": "python",
   "name": "spark_python_kubernetes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
